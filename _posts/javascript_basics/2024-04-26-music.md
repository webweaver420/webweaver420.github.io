---
layout: post
title: phyton 파이썬을 이용해서 뮤직차트 데이터 불러오는 방법
date: 2024-04-25 17:29 +0900
description: phyton
image: ../assets/img/img19.jpg
category: 코딩
tags: phyton 파이썬 뮤직차트 데이터
published: true
sitemap: true
---

안녕하세요.
오늘은 phyton을 이용해서 기존에 있는 뮤직사이트에서 차트 데이터를 수집 해 올건데요.
차트 수집 전에 먼저 Vs code를 실행해서
ctrl+shift+` 을 누르고 아래 4가지를 설치 합니다.

````python
1. pip install requests:
requests는 파이썬에서 HTTP 요청을 보내고 받는 데 사용되는 라이브러리입니다. 
이를 통해 웹페이지를 가져오고 웹 서버와 통신할 수 있습니다.
2. pip install beautifulsoup4:
beautifulsoup4는 HTML 및 XML 문서를 파싱하고 원하는 정보를 추출하는 파이썬 라이브러리입니다. 
웹 스크래핑 작업에서 많이 사용됩니다.
3. pip install lxml:
lxml은 파이썬에서 XML 및 HTML을 파싱하기 위한 라이브러리입니다. beautifulsoup4와 함께 사용되어 HTML 문서를 파싱하는 데 사용될 수 있습니다. 
일반적으로 lxml은 beautifulsoup4의 백엔드 파서로 사용됩니다.
4. pip install pandas:
pandas는 데이터 조작 및 분석을 위한 파이썬 라이브러리입니다. 표 형태의 데이터를 다루는 데 유용하며, 데이터를 읽고 쓰는 기능, 
데이터 정제 및 변환 기능 등을 제공합니다.
````

위 명령어를 실행하면 해당 라이브러리들이 파이썬 환경에 설치됩니다.

설치를 하셨다면 이제 본격적으로 뮤직차트를 수집 해 볼게요.

````python
import requests
from bs4 import BeautifulSoup

def get_bugs_music_chart():
    url = "https://music.bugs.co.kr/chart"
    response = requests.get(url)
    
    if response.status_code == 200:
        html = response.text
        soup = BeautifulSoup(html, "html.parser")
        
        # 차트에서 곡 정보를 담고 있는 요소를 선택합니다.
        songs = soup.select("table.list.trackList.byChart > tbody > tr")
        
        chart = []
        for song in songs:
            # 곡 제목 가져오기
            title = song.select_one("p.title > a").text.strip()
            # 가수 이름 가져오기
            artist = song.select_one("p.artist > a").text.strip()
            # 앨범 이름 가져오기
            album = song.select_one("a.album").text.strip()
            
            # 곡 정보를 딕셔너리로 저장합니다.
            song_info = {
                "title": title,
                "artist": artist,
                "album": album
            }
            chart.append(song_info)
        
        return chart
    else:
        print("Failed to fetch data from Bugs Music")

# 벅스뮤직 실시간 TOP 100 차트 가져오기
bugs_chart = get_bugs_music_chart()

# 결과 출력
for idx, song in enumerate(bugs_chart, start=1):
    print(f"{idx}. {song['title']} - {song['artist']} ({song['album']})")
````

코드를 확인하셨다면 아래는 코드 하나 하나 살펴보겠습니다.
먼저 import requests와 from bs4 import BeautifulSoup:
이 두 줄은 각각 requests와 BeautifulSoup 라이브러리를 가져옵니다. 
requests는 웹페이지에 HTTP 요청을 보내고 응답을 받는 데 사용되며, BeautifulSoup은 HTML 및 XML 문서를 파싱하고 원하는 정보를 추출하는 데 사용됩니다.

def get_bugs_music_chart()::
get_bugs_music_chart라는 이름의 함수를 정의합니다. 
이 함수는 벅스뮤직의 차트 정보를 가져오는 역할을 합니다.

URL 설정:
url = "https://music.bugs.co.kr/chart": 벅스뮤직의 실시간 차트 URL을 변수에 저장합니다.

HTTP 요청 보내기:
response = requests.get(url): 설정한 URL로 HTTP GET 요청을 보냅니다. 
그 결과를 response 변수에 저장합니다.

HTTP 상태 코드 확인:
if response.status_code == 200:: 
response 객체의 status_code 속성을 확인하여 HTTP 요청이 성공적으로 이루어졌는지를 확인합니다. 여기서 200은 "성공"을 의미합니다.

웹 페이지 파싱:
html = response.text: 
요청한 웹페이지의 HTML 내용을 html 변수에 저장합니다.

soup = BeautifulSoup(html, "html.parser"): 
BeautifulSoup을 사용하여 HTML을 파싱하고, 이를 soup 변수에 저장합니다.

곡 정보 추출:
songs = soup.select("table.list.trackList.byChart > tbody > tr"): 
BeautifulSoup의 select 메서드를 사용하여 웹페이지에서 곡 정보를 담고 있는 요소들을 선택합니다.

for in 반복문을 통한 곡 정보 추출:
for song in songs:: 
선택한 곡 정보 요소들을 반복하여 각 곡의 제목, 가수, 앨범 정보를 추출합니다.

title = song.select_one("p.title > a").text.strip(): 
각 곡의 제목을 추출합니다. select_one 메서드는 해당 선택자에 맞는 첫 번째 요소를 선택하고, text.strip()를 사용하여 텍스트를 가져옵니다.

artist = song.select_one("p.artist > a").text.strip(): 
각 곡의 가수를 추출합니다.

album = song.select_one("a.album").text.strip(): 
각 곡의 앨범을 추출합니다.

곡 정보 저장:
song_info 딕셔너리를 생성하여 각 곡의 정보를 저장합니다.

chart.append(song_info): 
chart 리스트에 각 곡의 정보를 저장합니다.

함수 반환:
return chart: 
곡 정보가 담긴 chart 리스트를 반환합니다.

결과 출력:
bugs_chart = get_bugs_music_chart(): 
앞서 정의한 함수를 호출하여 벅스뮤직의 실시간 TOP 100 차트 정보를 가져옵니다.

for idx, song in enumerate(bugs_chart, start=1):: 
enumerate 함수를 사용하여 bugs_chart 리스트의 각 요소에 대해 인덱스와 함께 반복합니다.

print(f"{idx}. {song['title']} - {song['artist']} ({song['album']})"): 
각 곡의 제목, 가수, 앨범 정보를 포맷에 맞게 출력합니다.


### status code 확인

````python
response = requests.get(url)

# HTTP 상태 코드 확인
if response.status_code == 200:
    # 요청이 성공적으로 이루어짐
    print("요청이 성공적으로 처리되었습니다.")
else:
    # 요청이 실패했거나 다른 문제가 발생함
    print("요청이 실패하였거나 다른 문제가 발생했습니다. 상태 코드:", response.status_code)
````

### 요청에 실패했을 경우

````python
head = {'user-Agent': "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15"}
````

HTTP 요청 헤더를 설정해서 사용자 에이전트를 지정하여 웹사이트에 접속하는 것처럼 보이도록 합니다.
이 경우는 다 똑같은 형식인지는 모르겠지만, 배운대로 정리해봅니다.
